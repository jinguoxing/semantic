/**
 * LLM Service - Main Service Class
 * LLM æœåŠ¡ä¸»ç±»ï¼Œè´Ÿè´£ä¸åç«¯ API é€šä¿¡
 */

import {
    ScenarioAnalysisRequest,
    ScenarioAnalysisResult,
    PolicyParseRequest,
    PolicyParseResult,
    GenerateObjectsRequest,
    GenerateObjectsResult,
    LLMResponse,
} from './types';
import { getLLMConfig } from './config';
import { mockAnalysisResultVNext } from '../../data/mockAnalysisVNext';
import { MOCK_RECOGNITION_RUN } from '../../data/mockRecognitionRun';
import { RecognitionRun } from '../../types/scene-model';

class LLMService {
    /**
     * åˆ†æåœºæ™¯æè¿°ï¼Œæå–ä¸šåŠ¡è¦ç´ 
     */
    async analyzeScenario(
        description: string,
        context?: string
    ): Promise<ScenarioAnalysisResult> {
        const config = getLLMConfig();
        const request: ScenarioAnalysisRequest = { description, context };

        try {
            const response = await fetch(`${config.baseUrl}/analyze-scenario`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    ...request,
                    provider: config.provider,
                    model: config.model,
                }),
                signal: AbortSignal.timeout(config.timeout || 30000),
            });

            // å¦‚æœæ˜¯404ï¼Œè¯´æ˜åç«¯æœªå®ç°ï¼Œä½¿ç”¨fallback
            if (response.status === 404) {
                console.warn('ğŸ”„ Backend API not implemented, using mock data fallback');
                return this.mockAnalyzeScenario(description);
            }

            if (!response.ok) {
                throw new Error(`HTTP ${response.status}: ${response.statusText}`);
            }

            const result: LLMResponse<ScenarioAnalysisResult> = await response.json();

            if (!result.success || !result.data) {
                throw new Error(result.error?.message || 'Analysis failed');
            }

            return result.data;
        } catch (error) {
            console.error('LLM Service - analyzeScenario error:', error);

            // ç½‘ç»œé”™è¯¯ä¹Ÿfallbackåˆ°mock
            if (error instanceof TypeError) {
                console.warn('ğŸ”„ Network error, using mock data fallback');
                return this.mockAnalyzeScenario(description);
            }

            throw this.handleError(error);
        }
    }

    /**
     * Phase 2: åŸºäºæ¨¡æ¿çš„åœºæ™¯åˆ†æ (Strict JSON Schema)
     */
    async analyzeScenarioV2(
        description: string,
        templateId: string,
        context?: string
    ): Promise<RecognitionRun> {
        try {
            // Simulate API call
            await new Promise(resolve => setTimeout(resolve, 2000)); // Latency

            // Return Mock Data (Sprint 1 MVP)
            console.log(`[Mock] Analyzing with template: ${templateId}`);
            return Promise.resolve(MOCK_RECOGNITION_RUN);

        } catch (error) {
            console.error('LLM Service - analyzeScenarioV2 error:', error);
            throw this.handleError(error);
        }
    }

    /**
     * è§£ææ”¿ç­–æ–‡ä»¶ï¼Œæå–å¤šä¸ªåœºæ™¯
     */
    async parsePolicy(file: File): Promise<PolicyParseResult> {
        const config = getLLMConfig();

        try {
            // Step 1: ä¸Šä¼ æ–‡ä»¶
            const uploadFormData = new FormData();
            uploadFormData.append('file', file);

            const uploadResponse = await fetch(`${config.baseUrl}/upload-policy`, {
                method: 'POST',
                body: uploadFormData,
            });

            if (!uploadResponse.ok) {
                throw new Error(`Upload failed: ${uploadResponse.statusText}`);
            }

            const uploadResult = await uploadResponse.json();
            const fileUrl = uploadResult.data?.fileUrl;

            if (!fileUrl) {
                throw new Error('File upload failed: no file URL returned');
            }

            // Step 2: è§£ææ–‡ä»¶
            const parseRequest: PolicyParseRequest = {
                fileUrl,
                fileName: file.name,
                fileType: this.getFileType(file.name),
            };

            const parseResponse = await fetch(`${config.baseUrl}/parse-policy`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    ...parseRequest,
                    provider: config.provider,
                    model: config.model,
                }),
                signal: AbortSignal.timeout(120000), // 2åˆ†é’Ÿè¶…æ—¶ï¼Œæ–‡ä»¶è§£æå¯èƒ½è¾ƒæ…¢
            });

            if (!parseResponse.ok) {
                throw new Error(`Parse failed: ${parseResponse.statusText}`);
            }

            const result: LLMResponse<PolicyParseResult> = await parseResponse.json();

            if (!result.success || !result.data) {
                throw new Error(result.error?.message || 'Policy parsing failed');
            }

            return result.data;
        } catch (error) {
            console.error('LLM Service - parsePolicy error:', error);
            throw this.handleError(error);
        }
    }

    /**
     * æ ¹æ®æå–çš„è¦ç´ ç”Ÿæˆä¸šåŠ¡å¯¹è±¡å»ºè®®
     */
    async generateBusinessObjects(
        request: GenerateObjectsRequest
    ): Promise<GenerateObjectsResult> {
        const config = getLLMConfig();

        try {
            const response = await fetch(`${config.baseUrl}/generate-objects`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    ...request,
                    provider: config.provider,
                    model: config.model,
                }),
                signal: AbortSignal.timeout(config.timeout || 30000),
            });

            if (!response.ok) {
                throw new Error(`HTTP ${response.status}: ${response.statusText}`);
            }

            const result: LLMResponse<GenerateObjectsResult> = await response.json();

            if (!result.success || !result.data) {
                throw new Error(result.error?.message || 'Object generation failed');
            }

            return result.data;
        } catch (error) {
            console.error('LLM Service - generateBusinessObjects error:', error);
            throw this.handleError(error);
        }
    }



    /**
     * Mock åœºæ™¯åˆ†æï¼ˆå½“åç«¯APIä¸å¯ç”¨æ—¶çš„fallbackï¼‰
     */
    private mockAnalyzeScenario(description: string): ScenarioAnalysisResult {
        // æ¨¡æ‹Ÿåˆ†æå»¶è¿Ÿ
        console.log('Mock analyzing scenario:', description.substring(0, 50) + '...');
        return mockAnalysisResultVNext;
    }

    /**
     * é”™è¯¯å¤„ç†
     */
    private handleError(error: any): Error {
        if (error.name === 'TimeoutError' || error.name === 'AbortError') {
            return new Error('è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•');
        }

        if (error.message) {
            return error;
        }

        return new Error('æœªçŸ¥é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜');
    }

    /**
     * è·å–æ–‡ä»¶ç±»å‹
     */
    private getFileType(fileName: string): 'pdf' | 'docx' | 'image' {
        const ext = fileName.split('.').pop()?.toLowerCase();
        if (ext === 'pdf') return 'pdf';
        if (ext === 'doc' || ext === 'docx') return 'docx';
        return 'image';
    }
}

// å¯¼å‡ºå•ä¾‹
export const llmService = new LLMService();

// å¯¼å‡ºç±»ä¾›æµ‹è¯•ä½¿ç”¨
export default LLMService;
